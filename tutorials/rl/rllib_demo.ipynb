{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c9226e-dd4e-47ee-ad12-ca679bda457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67151d6-9064-4a72-8e53-0727c19ce306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.metrics import ENV_RUNNER_RESULTS, EPISODE_RETURN_MEAN\n",
    "\n",
    "from flatland.ml.ray.examples.flatland_inference_with_random_policy import add_flatland_inference_with_random_policy_args, rollout\n",
    "from flatland.ml.ray.examples.flatland_training_with_parameter_sharing import train, add_flatland_training_with_parameter_sharing_args, \\\n",
    "    register_flatland_ray_cli_observation_builders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6a582-93d4-4714-8a1c-618671911851",
   "metadata": {},
   "source": [
    "# RLlib\n",
    "RLlib (https://arxiv.org/abs/1712.09381) is an open source library for reinforcement learning (RL), offering support for production-level, highly scalable, and fault-tolerant RL workloads, while maintaining simple and unified APIs for a large variety of industry applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d3c1c8-4ee9-41d3-baf3-2a5ae9472d64",
   "metadata": {},
   "source": [
    "### Register observation builds in rllib input registry\n",
    "\n",
    "These are the registered keys you can use for the `--obs-builder` param below. Use `regiser_input` to register your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857cd9c0-d314-448f-b2c9-ae9de456ccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mregister_flatland_ray_cli_observation_builders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregister_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DummyObservationBuilderGym\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDummyObservationBuilderGym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregister_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GlobalObsForRailEnvGym\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGlobalObsForRailEnvGym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregister_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FlattenedNormalizedTreeObsForRailEnv_max_depth_3_50\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                   \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFlattenedNormalizedTreeObsForRailEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mShortestPathPredictorForRailEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat inspect.getsource(register_flatland_ray_cli_observation_builders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a60a468-c976-4695-896f-e02cb9357980",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_flatland_ray_cli_observation_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600bc3b-a6bd-4b5d-9332-ec09729eb9be",
   "metadata": {},
   "source": [
    "## Rllib Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4325e0fc-7012-4b47-a0db-974e4fe979b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = add_flatland_training_with_parameter_sharing_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749441b0-9005-4846-a3d5-2d326d2adbfc",
   "metadata": {},
   "source": [
    "#### Inspect Training cli Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b525ce9-fc42-41dd-bef2-24250e2efe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: flatland_training_with_parameter_sharing.py [-h] [--algo ALGO]\n",
      "                                                   [--enable-new-api-stack]\n",
      "                                                   [--framework {tf,tf2,torch}]\n",
      "                                                   [--env ENV]\n",
      "                                                   [--num-env-runners NUM_ENV_RUNNERS]\n",
      "                                                   [--num-envs-per-env-runner NUM_ENVS_PER_ENV_RUNNER]\n",
      "                                                   [--num-agents NUM_AGENTS]\n",
      "                                                   [--evaluation-num-env-runners EVALUATION_NUM_ENV_RUNNERS]\n",
      "                                                   [--evaluation-interval EVALUATION_INTERVAL]\n",
      "                                                   [--evaluation-duration EVALUATION_DURATION]\n",
      "                                                   [--evaluation-duration-unit {episodes,timesteps}]\n",
      "                                                   [--evaluation-parallel-to-training]\n",
      "                                                   [--output OUTPUT]\n",
      "                                                   [--log-level {INFO,DEBUG,WARN,ERROR}]\n",
      "                                                   [--no-tune]\n",
      "                                                   [--num-samples NUM_SAMPLES]\n",
      "                                                   [--max-concurrent-trials MAX_CONCURRENT_TRIALS]\n",
      "                                                   [--verbose VERBOSE]\n",
      "                                                   [--checkpoint-freq CHECKPOINT_FREQ]\n",
      "                                                   [--checkpoint-at-end]\n",
      "                                                   [--wandb-key WANDB_KEY]\n",
      "                                                   [--wandb-project WANDB_PROJECT]\n",
      "                                                   [--wandb-run-name WANDB_RUN_NAME]\n",
      "                                                   [--stop-reward STOP_REWARD]\n",
      "                                                   [--stop-iters STOP_ITERS]\n",
      "                                                   [--stop-timesteps STOP_TIMESTEPS]\n",
      "                                                   [--as-test]\n",
      "                                                   [--as-release-test]\n",
      "                                                   [--num-learners NUM_LEARNERS]\n",
      "                                                   [--num-gpus-per-learner NUM_GPUS_PER_LEARNER]\n",
      "                                                   [--num-aggregator-actors-per-learner NUM_AGGREGATOR_ACTORS_PER_LEARNER]\n",
      "                                                   [--num-cpus NUM_CPUS]\n",
      "                                                   [--local-mode]\n",
      "                                                   [--num-gpus NUM_GPUS]\n",
      "                                                   [--train-batch-size-per-learner TRAIN_BATCH_SIZE_PER_LEARNER]\n",
      "                                                   [--obs-builder OBS_BUILDER]\n",
      "                                                   [--ray-address RAY_ADDRESS]\n",
      "                                                   [--env_var [KEY=VALUE ...]]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --algo ALGO           The RLlib-registered algorithm to use.\n",
      "  --enable-new-api-stack\n",
      "                        Whether to use the `enable_rl_module_and_learner`\n",
      "                        config setting.\n",
      "  --framework {tf,tf2,torch}\n",
      "                        The DL framework specifier.\n",
      "  --env ENV             The gym.Env identifier to run the experiment with.\n",
      "  --num-env-runners NUM_ENV_RUNNERS\n",
      "                        The number of (remote) EnvRunners to use for the\n",
      "                        experiment.\n",
      "  --num-envs-per-env-runner NUM_ENVS_PER_ENV_RUNNER\n",
      "                        The number of (vectorized) environments per EnvRunner.\n",
      "                        Note that this is identical to the batch size for\n",
      "                        (inference) action computations.\n",
      "  --num-agents NUM_AGENTS\n",
      "                        If 0 (default), will run as single-agent. If > 0, will\n",
      "                        run as multi-agent with the environment simply cloned\n",
      "                        n times and each agent acting independently at every\n",
      "                        single timestep. The overall reward for this\n",
      "                        experiment is then the sum over all individual agents'\n",
      "                        rewards.\n",
      "  --evaluation-num-env-runners EVALUATION_NUM_ENV_RUNNERS\n",
      "                        The number of evaluation (remote) EnvRunners to use\n",
      "                        for the experiment.\n",
      "  --evaluation-interval EVALUATION_INTERVAL\n",
      "                        Every how many iterations to run one round of\n",
      "                        evaluation. Use 0 (default) to disable evaluation.\n",
      "  --evaluation-duration EVALUATION_DURATION\n",
      "                        The number of evaluation units to run each evaluation\n",
      "                        round. Use `--evaluation-duration-unit` to count\n",
      "                        either in 'episodes' or 'timesteps'. If 'auto', will\n",
      "                        run as many as possible during train pass\n",
      "                        (`--evaluation-parallel-to-training` must be set\n",
      "                        then).\n",
      "  --evaluation-duration-unit {episodes,timesteps}\n",
      "                        The evaluation duration unit to count by. One of\n",
      "                        'episodes' or 'timesteps'. This unit will be run\n",
      "                        `--evaluation-duration` times in each evaluation\n",
      "                        round. If `--evaluation-duration=auto`, this setting\n",
      "                        does not matter.\n",
      "  --evaluation-parallel-to-training\n",
      "                        Whether to run evaluation parallel to training. This\n",
      "                        might help speed up your overall iteration time. Be\n",
      "                        aware that when using this option, your reported\n",
      "                        evaluation results are referring to one iteration\n",
      "                        before the current one.\n",
      "  --output OUTPUT       The output directory to write trajectories to, which\n",
      "                        are collected by the algo's EnvRunners.\n",
      "  --log-level {INFO,DEBUG,WARN,ERROR}\n",
      "                        The log-level to be used by the RLlib logger.\n",
      "  --no-tune             Whether to NOT use tune.Tuner(), but rather a simple\n",
      "                        for-loop calling `algo.train()` repeatedly until one\n",
      "                        of the stop criteria is met.\n",
      "  --num-samples NUM_SAMPLES\n",
      "                        How many (tune.Tuner.fit()) experiments to execute -\n",
      "                        if possible in parallel.\n",
      "  --max-concurrent-trials MAX_CONCURRENT_TRIALS\n",
      "                        How many (tune.Tuner) trials to run concurrently.\n",
      "  --verbose VERBOSE     The verbosity level for the `tune.Tuner()` running the\n",
      "                        experiment.\n",
      "  --checkpoint-freq CHECKPOINT_FREQ\n",
      "                        The frequency (in training iterations) with which to\n",
      "                        create checkpoints. Note that if --wandb-key is\n",
      "                        provided, all checkpoints will automatically be\n",
      "                        uploaded to WandB.\n",
      "  --checkpoint-at-end   Whether to create a checkpoint at the very end of the\n",
      "                        experiment. Note that if --wandb-key is provided, all\n",
      "                        checkpoints will automatically be uploaded to WandB.\n",
      "  --wandb-key WANDB_KEY\n",
      "                        The WandB API key to use for uploading results.\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        The WandB project name to use.\n",
      "  --wandb-run-name WANDB_RUN_NAME\n",
      "                        The WandB run name to use.\n",
      "  --stop-reward STOP_REWARD\n",
      "                        Reward at which the script should stop training.\n",
      "  --stop-iters STOP_ITERS\n",
      "                        The number of iterations to train.\n",
      "  --stop-timesteps STOP_TIMESTEPS\n",
      "                        The number of (environment sampling) timesteps to\n",
      "                        train.\n",
      "  --as-test             Whether this script should be run as a test. If set,\n",
      "                        --stop-reward must be achieved within --stop-timesteps\n",
      "                        AND --stop-iters, otherwise this script will throw an\n",
      "                        exception at the end.\n",
      "  --as-release-test     Whether this script should be run as a release test.\n",
      "                        If set, all that applies to the --as-test option is\n",
      "                        true, plus, a short JSON summary will be written into\n",
      "                        a results file whose location is given by the ENV\n",
      "                        variable `TEST_OUTPUT_JSON`.\n",
      "  --num-learners NUM_LEARNERS\n",
      "                        The number of Learners to use. If `None`, use the\n",
      "                        algorithm's default value.\n",
      "  --num-gpus-per-learner NUM_GPUS_PER_LEARNER\n",
      "                        The number of GPUs per Learner to use. If `None` and\n",
      "                        there are enough GPUs for all required Learners\n",
      "                        (--num-learners), use a value of 1, otherwise 0.\n",
      "  --num-aggregator-actors-per-learner NUM_AGGREGATOR_ACTORS_PER_LEARNER\n",
      "                        The number of Aggregator actors to use per Learner. If\n",
      "                        `None`, use the algorithm's default value.\n",
      "  --num-cpus NUM_CPUS\n",
      "  --local-mode          Init Ray in local mode for easier debugging.\n",
      "  --num-gpus NUM_GPUS   The number of GPUs to use (only on the old API stack).\n",
      "  --train-batch-size-per-learner TRAIN_BATCH_SIZE_PER_LEARNER\n",
      "                        See https://docs.ray.io/en/latest/rllib/package_ref/do\n",
      "                        c/ray.rllib.algorithms.algorithm_config.AlgorithmConfi\n",
      "                        g.training.html#ray.rllib.algorithms.algorithm_config.\n",
      "                        AlgorithmConfig.training\n",
      "  --obs-builder OBS_BUILDER\n",
      "  --ray-address RAY_ADDRESS\n",
      "                        The address of the ray cluster to connect to in the\n",
      "                        form ray://<head_node_ip_address>:10001. Leave empty\n",
      "                        to start a new cluster. Passed to\n",
      "                        ray.init(address=...). See\n",
      "                        https://docs.ray.io/en/latest/ray-\n",
      "                        core/api/doc/ray.init.html\n",
      "  --env_var [KEY=VALUE ...], -e [KEY=VALUE ...]\n",
      "                        Set ray runtime environment variables like -e\n",
      "                        RAY_DEBUG=1, passed to ray.init(runtime_env={env_vars:\n",
      "                        {...}}), see https://docs.ray.io/en/latest/ray-\n",
      "                        core/handling-dependencies.html#api-reference\n"
     ]
    }
   ],
   "source": [
    "!python -m flatland.ml.ray.examples.flatland_training_with_parameter_sharing --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7feaf-a5ce-471e-b25a-9a7f5cac9920",
   "metadata": {},
   "source": [
    "#### Inspect Training cli Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef7b107-c753-463b-95e8-9df7ff1ccef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResultDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResultGrid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_flatland_training_with_parameter_sharing_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Must set --num-agents > 0 when running this script!\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_new_api_stack\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Must set --enable-new-api-stack when running this script!\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_builder\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Must set --obs-builder <obs builder ID> when running this script!\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msetup_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0minit_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0menv_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0menv_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"runtime_env\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"env_vars\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"worker_process_setup_hook\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"flatland.ml.ray.examples.flatland_training_with_parameter_sharing.setup_func\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"ignore_reinit_error\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mray_address\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minit_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mray_address\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"flatland_env\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mray_env_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_builder_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregistry_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_builder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# TODO could be extracted to cli - keep it low key as illustration only\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madditional_training_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DQN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0madditional_training_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"replay_buffer_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"MultiAgentEpisodeReplayBuffer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbase_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# N.B. the warning `passive_env_checker.py:164: UserWarning: WARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64`\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m#   comes from ray.tune.registry._register_all() -->  import ray.rllib.algorithms.dreamerv3 as dreamerv3!\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mget_trainable_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mget_default_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flatland_env\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mmulti_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# All agents map to the exact same policy.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpolicy_mapping_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0maid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"vf_share_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size_per_learner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0madditional_training_config\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mrl_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrl_module_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMultiRLModuleSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mrl_module_specs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRLModuleSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_rllib_example_script_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_errors\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat inspect.getsource(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c56309-42fc-4677-88c5-3824251d3977",
   "metadata": {},
   "source": [
    "#### Run Training with PPO for one iteration with reduced batch size and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841aff0a-3f94-457f-9c22-92ea11cf085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/che/workspaces/flatland-rl-2/notebooks\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e126a368-a5dc-451c-a5eb-82e6c5b25fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:51:26,759\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "/Users/che/Miniconda3/miniconda3/envs/flatland/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/che/Miniconda3/miniconda3/envs/flatland/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/che/Miniconda3/miniconda3/envs/flatland/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/che/Miniconda3/miniconda3/envs/flatland/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2025-03-21 16:51:28,339\tINFO worker.py:1672 -- Calling ray.init() again after it has already been called.\n",
      "2025-03-21 16:51:28,384\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-03-21 16:51:28,421\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:28 (running for 00:00:00.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+------------------------------+----------+-------+\n",
      "| Trial name                   | status   | loc   |\n",
      "|------------------------------+----------+-------|\n",
      "| PPO_flatland_env_59ef6_00000 | PENDING  |       |\n",
      "+------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:33 (running for 00:00:05.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+------------------------------+----------+-------+\n",
      "| Trial name                   | status   | loc   |\n",
      "|------------------------------+----------+-------|\n",
      "| PPO_flatland_env_59ef6_00000 | PENDING  |       |\n",
      "+------------------------------+----------+-------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-03-21 16:51:36,766 E 85004 3531068] file_system_monitor.cc:116: /tmp/ray/session_2025-03-21_16-51-24_808186_84981 is over 95% full, available space: 86.4491 GB; capacity: 1858.19 GB. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:38 (running for 00:00:10.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+------------------------------+----------+-------+\n",
      "| Trial name                   | status   | loc   |\n",
      "|------------------------------+----------+-------|\n",
      "| PPO_flatland_env_59ef6_00000 | PENDING  |       |\n",
      "+------------------------------+----------+-------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=85022)\u001b[0m 2025-03-21 16:51:39,841\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:43 (running for 00:00:15.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+------------------------------+----------+-------+\n",
      "| Trial name                   | status   | loc   |\n",
      "|------------------------------+----------+-------|\n",
      "| PPO_flatland_env_59ef6_00000 | PENDING  |       |\n",
      "+------------------------------+----------+-------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m /Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:321: UserWarning: Could not set all required cities! Created 1/2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m   warnings.warn(city_warning)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m /Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:217: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m   warnings.warn(\"[WARNING] Changing to Grid mode to place at least 2 cities.\")\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m 2025-03-21 16:51:46,093\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m 2025-03-21 16:51:46,093\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Discrete(5), Discrete(5)]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Box(0.0, 2.0, (1020,), float64), Box(0.0, 2.0, (1020,), float64)]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Discrete(5), Discrete(5)]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Box(0.0, 2.0, (1020,), float64), Box(0.0, 2.0, (1020,), float64)]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Discrete(5), Discrete(5)]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85026)\u001b[0m agent_to_module_mapping.py: [Box(0.0, 2.0, (1020,), float64), Box(0.0, 2.0, (1020,), float64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=85022)\u001b[0m 2025-03-21 16:51:46,491\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[33m(raylet)\u001b[0m [2025-03-21 16:51:46,860 E 85004 3531068] file_system_monitor.cc:116: /tmp/ray/session_2025-03-21_16-51-24_808186_84981 is over 95% full, available space: 86.4488 GB; capacity: 1858.19 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(PPO pid=85022)\u001b[0m Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:48 (running for 00:00:20.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+-----------------+\n",
      "| Trial name                   | status   | loc             |\n",
      "|------------------------------+----------+-----------------|\n",
      "| PPO_flatland_env_59ef6_00000 | RUNNING  | 127.0.0.1:85022 |\n",
      "+------------------------------+----------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:51:50,270\tWARNING trial.py:863 -- Stopping criterion 'env_runners/episode_return_mean' not found in result dict! Available keys are ['num_training_step_calls_per_iteration', 'num_env_steps_sampled_lifetime', 'num_env_steps_sampled_lifetime_throughput', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore', 'timers/training_iteration', 'timers/restore_env_runners', 'timers/training_step', 'timers/env_runner_sampling_timer', 'timers/learner_update_timer', 'timers/synch_weights', 'env_runners/num_env_steps_sampled', 'env_runners/num_env_steps_sampled_lifetime', 'env_runners/env_to_module_sum_episodes_length_in', 'env_runners/env_to_module_sum_episodes_length_out', 'env_runners/num_env_steps_sampled_lifetime_throughput', 'fault_tolerance/num_healthy_workers', 'fault_tolerance/num_remote_worker_restarts', 'env_runner_group/actor_manager_num_outstanding_async_reqs', 'config/placement_strategy', 'config/num_gpus', 'config/_fake_gpus', 'config/num_cpus_for_main_process', 'config/eager_tracing', 'config/eager_max_retraces', 'config/torch_compile_learner', 'config/torch_compile_learner_what_to_compile', 'config/torch_compile_learner_dynamo_backend', 'config/torch_compile_learner_dynamo_mode', 'config/torch_compile_worker', 'config/torch_compile_worker_dynamo_backend', 'config/torch_compile_worker_dynamo_mode', 'config/torch_skip_nan_gradients', 'config/env', 'config/observation_space', 'config/action_space', 'config/clip_rewards', 'config/normalize_actions', 'config/clip_actions', 'config/_is_atari', 'config/disable_env_checking', 'config/render_env', 'config/action_mask_key', 'config/env_runner_cls', 'config/num_env_runners', 'config/num_envs_per_env_runner', 'config/gym_env_vectorize_mode', 'config/num_cpus_per_env_runner', 'config/num_gpus_per_env_runner', 'config/validate_env_runners_after_construction', 'config/episodes_to_numpy', 'config/max_requests_in_flight_per_env_runner', 'config/sample_timeout_s', 'config/_env_to_module_connector', 'config/add_default_connectors_to_env_to_module_pipeline', 'config/_module_to_env_connector', 'config/add_default_connectors_to_module_to_env_pipeline', 'config/episode_lookback_horizon', 'config/rollout_fragment_length', 'config/batch_mode', 'config/compress_observations', 'config/remote_worker_envs', 'config/remote_env_batch_wait_ms', 'config/enable_tf1_exec_eagerly', 'config/sample_collector', 'config/preprocessor_pref', 'config/observation_filter', 'config/update_worker_filter_stats', 'config/use_worker_filter_stats', 'config/sampler_perf_stats_ema_coef', 'config/num_learners', 'config/num_gpus_per_learner', 'config/num_cpus_per_learner', 'config/num_aggregator_actors_per_learner', 'config/max_requests_in_flight_per_aggregator_actor', 'config/local_gpu_idx', 'config/max_requests_in_flight_per_learner', 'config/gamma', 'config/lr', 'config/grad_clip', 'config/grad_clip_by', 'config/_train_batch_size_per_learner', 'config/train_batch_size', 'config/num_epochs', 'config/minibatch_size', 'config/shuffle_batch_per_epoch', 'config/_learner_connector', 'config/add_default_connectors_to_learner_pipeline', 'config/_learner_class', 'config/callbacks_on_algorithm_init', 'config/callbacks_on_env_runners_recreated', 'config/callbacks_on_checkpoint_loaded', 'config/callbacks_on_environment_created', 'config/callbacks_on_episode_created', 'config/callbacks_on_episode_start', 'config/callbacks_on_episode_step', 'config/callbacks_on_episode_end', 'config/callbacks_on_evaluate_start', 'config/callbacks_on_evaluate_end', 'config/callbacks_on_sample_end', 'config/callbacks_on_train_result', 'config/explore', 'config/enable_rl_module_and_learner', 'config/enable_env_runner_and_connector_v2', 'config/count_steps_by', 'config/policy_map_capacity', 'config/policy_mapping_fn', 'config/policies_to_train', 'config/policy_states_are_swappable', 'config/observation_fn', 'config/offline_data_class', 'config/input_read_method', 'config/input_read_episodes', 'config/input_read_sample_batches', 'config/input_read_batch_size', 'config/input_filesystem', 'config/input_compress_columns', 'config/input_spaces_jsonable', 'config/materialize_data', 'config/materialize_mapped_data', 'config/prelearner_class', 'config/prelearner_buffer_class', 'config/prelearner_module_synch_period', 'config/dataset_num_iters_per_learner', 'config/actions_in_input_normalized', 'config/postprocess_inputs', 'config/shuffle_buffer_size', 'config/output', 'config/output_compress_columns', 'config/output_max_file_size', 'config/output_max_rows_per_file', 'config/output_write_remaining_data', 'config/output_write_method', 'config/output_filesystem', 'config/output_write_episodes', 'config/offline_sampling', 'config/evaluation_interval', 'config/evaluation_duration', 'config/evaluation_duration_unit', 'config/evaluation_sample_timeout_s', 'config/evaluation_parallel_to_training', 'config/evaluation_force_reset_envs_before_iteration', 'config/evaluation_config', 'config/ope_split_batch_by_episode', 'config/evaluation_num_env_runners', 'config/in_evaluation', 'config/sync_filters_on_rollout_workers_timeout_s', 'config/keep_per_episode_custom_metrics', 'config/metrics_episode_collection_timeout_s', 'config/metrics_num_episodes_for_smoothing', 'config/min_time_s_per_iteration', 'config/min_train_timesteps_per_iteration', 'config/min_sample_timesteps_per_iteration', 'config/log_gradients', 'config/export_native_model_files', 'config/checkpoint_trainable_policies_only', 'config/logger_creator', 'config/logger_config', 'config/log_level', 'config/log_sys_usage', 'config/fake_sampler', 'config/seed', 'config/restart_failed_env_runners', 'config/ignore_env_runner_failures', 'config/max_num_env_runner_restarts', 'config/delay_between_env_runner_restarts_s', 'config/restart_failed_sub_environments', 'config/num_consecutive_env_runner_failures_tolerance', 'config/env_runner_health_probe_timeout_s', 'config/env_runner_restore_timeout_s', 'config/_rl_module_spec', 'config/_validate_config', 'config/_use_msgpack_checkpoints', 'config/_torch_grad_scaler_class', 'config/_torch_lr_scheduler_classes', 'config/_tf_policy_handles_more_than_one_loss', 'config/_disable_preprocessor_api', 'config/_disable_action_flattening', 'config/_disable_initialize_loss_from_dummy_batch', 'config/_dont_auto_sync_env_runner_states', 'config/env_task_fn', 'config/enable_connectors', 'config/simple_optimizer', 'config/policy_map_cache', 'config/worker_cls', 'config/synchronize_filters', 'config/enable_async_evaluation', 'config/custom_async_evaluation_function', 'config/_enable_rl_module_api', 'config/auto_wrap_old_gym_envs', 'config/always_attach_evaluation_results', 'config/replay_sequence_length', 'config/_disable_execution_plan_api', 'config/use_critic', 'config/use_gae', 'config/use_kl_loss', 'config/kl_coeff', 'config/kl_target', 'config/vf_loss_coeff', 'config/entropy_coeff', 'config/clip_param', 'config/vf_clip_param', 'config/entropy_coeff_schedule', 'config/lr_schedule', 'config/sgd_minibatch_size', 'config/vf_share_layers', 'config/__stdout_file__', 'config/__stderr_file__', 'config/lambda', 'config/input', 'config/callbacks', 'config/create_env_on_driver', 'config/custom_eval_function', 'config/framework', 'perf/cpu_util_percent', 'perf/ram_util_percent', 'env_runners/num_agent_steps_sampled_lifetime/0', 'env_runners/num_agent_steps_sampled_lifetime/1', 'env_runners/num_module_steps_sampled/p0', 'env_runners/num_agent_steps_sampled/1', 'env_runners/num_agent_steps_sampled/0', 'env_runners/num_module_steps_sampled_lifetime/p0', 'learners/__all_modules__/num_non_trainable_parameters', 'learners/__all_modules__/learner_connector_sum_episodes_length_in', 'learners/__all_modules__/learner_connector_sum_episodes_length_out', 'learners/__all_modules__/num_env_steps_trained_lifetime', 'learners/__all_modules__/num_trainable_parameters', 'learners/__all_modules__/num_module_steps_trained', 'learners/__all_modules__/num_env_steps_trained', 'learners/__all_modules__/num_module_steps_trained_lifetime', 'learners/__all_modules__/num_env_steps_trained_lifetime_throughput', 'learners/p0/num_trainable_parameters', 'learners/p0/weights_seq_no', 'learners/p0/entropy', 'learners/p0/module_train_batch_size_mean', 'learners/p0/policy_loss', 'learners/p0/total_loss', 'learners/p0/vf_loss_unclipped', 'learners/p0/num_module_steps_trained', 'learners/p0/mean_kl_loss', 'learners/p0/num_module_steps_trained_lifetime', 'learners/p0/curr_kl_coeff', 'learners/p0/num_non_trainable_parameters', 'learners/p0/vf_explained_var', 'learners/p0/default_optimizer_learning_rate', 'learners/p0/gradients_default_optimizer_global_norm', 'learners/p0/curr_entropy_coeff', 'learners/p0/diff_num_grad_updates_vs_sampler_policy', 'learners/p0/vf_loss', 'config/tf_session_args/intra_op_parallelism_threads', 'config/tf_session_args/inter_op_parallelism_threads', 'config/tf_session_args/log_device_placement', 'config/tf_session_args/allow_soft_placement', 'config/local_tf_session_args/intra_op_parallelism_threads', 'config/local_tf_session_args/inter_op_parallelism_threads', 'config/model/fcnet_hiddens', 'config/model/fcnet_activation', 'config/model/fcnet_weights_initializer', 'config/model/fcnet_weights_initializer_config', 'config/model/fcnet_bias_initializer', 'config/model/fcnet_bias_initializer_config', 'config/model/conv_filters', 'config/model/conv_activation', 'config/model/conv_kernel_initializer', 'config/model/conv_kernel_initializer_config', 'config/model/conv_bias_initializer', 'config/model/conv_bias_initializer_config', 'config/model/conv_transpose_kernel_initializer', 'config/model/conv_transpose_kernel_initializer_config', 'config/model/conv_transpose_bias_initializer', 'config/model/conv_transpose_bias_initializer_config', 'config/model/post_fcnet_hiddens', 'config/model/post_fcnet_activation', 'config/model/post_fcnet_weights_initializer', 'config/model/post_fcnet_weights_initializer_config', 'config/model/post_fcnet_bias_initializer', 'config/model/post_fcnet_bias_initializer_config', 'config/model/free_log_std', 'config/model/log_std_clip_param', 'config/model/no_final_linear', 'config/model/vf_share_layers', 'config/model/use_lstm', 'config/model/max_seq_len', 'config/model/lstm_cell_size', 'config/model/lstm_use_prev_action', 'config/model/lstm_use_prev_reward', 'config/model/lstm_weights_initializer', 'config/model/lstm_weights_initializer_config', 'config/model/lstm_bias_initializer', 'config/model/lstm_bias_initializer_config', 'config/model/_time_major', 'config/model/use_attention', 'config/model/attention_num_transformer_units', 'config/model/attention_dim', 'config/model/attention_num_heads', 'config/model/attention_head_dim', 'config/model/attention_memory_inference', 'config/model/attention_memory_training', 'config/model/attention_position_wise_mlp_dim', 'config/model/attention_init_gru_gate_bias', 'config/model/attention_use_n_prev_actions', 'config/model/attention_use_n_prev_rewards', 'config/model/framestack', 'config/model/dim', 'config/model/grayscale', 'config/model/zero_mean', 'config/model/custom_model', 'config/model/custom_action_dist', 'config/model/custom_preprocessor', 'config/model/encoder_latent_dim', 'config/model/always_check_shapes', 'config/model/lstm_use_prev_action_reward', 'config/model/_use_default_native_models', 'config/model/_disable_preprocessor_api', 'config/model/_disable_action_flattening', 'config/_prior_exploration_config/type', 'config/policies/p0', 'env_runners/timers/connectors/AgentToModuleMapping', 'env_runners/timers/connectors/NumpyToTensor', 'env_runners/timers/connectors/UnBatchToIndividualItems', 'env_runners/timers/connectors/AddStatesFromEpisodesToBatch', 'env_runners/timers/connectors/TensorToNumpy', 'env_runners/timers/connectors/NormalizeAndClipActions', 'env_runners/timers/connectors/ModuleToAgentUnmapping', 'env_runners/timers/connectors/AddTimeDimToBatchAndZeroPad', 'env_runners/timers/connectors/ListifyDataForVectorEnv', 'env_runners/timers/connectors/AddObservationsFromEpisodesToBatch', 'env_runners/timers/connectors/GetActions', 'env_runners/timers/connectors/BatchIndividualItems', 'env_runners/timers/connectors/RemoveSingleTsTimeRankFromBatch', 'config/tf_session_args/gpu_options/allow_growth', 'config/tf_session_args/device_count/CPU', 'learners/__all_modules__/timers/connectors/BatchIndividualItems', 'learners/__all_modules__/timers/connectors/AddOneTsToEpisodesAndTruncate', 'learners/__all_modules__/timers/connectors/AgentToModuleMapping', 'learners/__all_modules__/timers/connectors/NumpyToTensor', 'learners/__all_modules__/timers/connectors/GeneralAdvantageEstimation', 'learners/__all_modules__/timers/connectors/AddStatesFromEpisodesToBatch', 'learners/__all_modules__/timers/connectors/AddTimeDimToBatchAndZeroPad', 'learners/__all_modules__/timers/connectors/AddObservationsFromEpisodesToBatch', 'learners/__all_modules__/timers/connectors/AddColumnsFromEpisodesToTrainBatch']. If 'env_runners/episode_return_mean' is never reported, the run will continue until training is finished.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>env_runner_group                               </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th>fault_tolerance                                            </th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime_throughput</th><th style=\"text-align: right;\">  num_training_step_calls_per_iteration</th><th>perf                                                    </th><th>timers                                                                                                                                                                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_flatland_env_59ef6_00000</td><td>{&#x27;actor_manager_num_outstanding_async_reqs&#x27;: 0}</td><td>{&#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;0&#x27;: 200, &#x27;1&#x27;: 200}, &#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;AgentToModuleMapping&#x27;: 7.898451490889527e-06, &#x27;NumpyToTensor&#x27;: 0.0006514624725359729, &#x27;UnBatchToIndividualItems&#x27;: 3.3355799842430625e-05, &#x27;AddStatesFromEpisodesToBatch&#x27;: 6.196388367066876e-06, &#x27;TensorToNumpy&#x27;: 0.0002189852724555016, &#x27;NormalizeAndClipActions&#x27;: 0.00017074410228305476, &#x27;ModuleToAgentUnmapping&#x27;: 6.448348645382307e-06, &#x27;AddTimeDimToBatchAndZeroPad&#x27;: 2.2233997937464932e-05, &#x27;ListifyDataForVectorEnv&#x27;: 8.073932804185945e-06, &#x27;AddObservationsFromEpisodesToBatch&#x27;: 3.9197153741186344e-05, &#x27;GetActions&#x27;: 0.020396071205466673, &#x27;BatchIndividualItems&#x27;: 5.00980064539732e-05, &#x27;RemoveSingleTsTimeRankFromBatch&#x27;: 2.0477231243917827e-06}}, &#x27;num_module_steps_sampled&#x27;: {&#x27;p0&#x27;: 400}, &#x27;num_env_steps_sampled&#x27;: 200, &#x27;num_agent_steps_sampled&#x27;: {&#x27;1&#x27;: 200, &#x27;0&#x27;: 200}, &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;p0&#x27;: 400}, &#x27;num_env_steps_sampled_lifetime&#x27;: 200, &#x27;env_to_module_sum_episodes_length_in&#x27;: 37.23720178604972, &#x27;env_to_module_sum_episodes_length_out&#x27;: 37.23720178604972, &#x27;num_env_steps_sampled_lifetime_throughput&#x27;: nan}</td><td>{&#x27;num_healthy_workers&#x27;: 2, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>{&#x27;__all_modules__&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;BatchIndividualItems&#x27;: 0.009110875020269305, &#x27;AddOneTsToEpisodesAndTruncate&#x27;: 0.0038188330072443932, &#x27;AgentToModuleMapping&#x27;: 0.000297874998068437, &#x27;NumpyToTensor&#x27;: 0.0008266670047305524, &#x27;GeneralAdvantageEstimation&#x27;: 0.017291625001234934, &#x27;AddStatesFromEpisodesToBatch&#x27;: 9.374984074383974e-06, &#x27;AddTimeDimToBatchAndZeroPad&#x27;: 3.024999750778079e-05, &#x27;AddObservationsFromEpisodesToBatch&#x27;: 8.495798101648688e-05, &#x27;AddColumnsFromEpisodesToTrainBatch&#x27;: 0.008209916995838284}}, &#x27;num_non_trainable_parameters&#x27;: 0, &#x27;learner_connector_sum_episodes_length_in&#x27;: 200, &#x27;learner_connector_sum_episodes_length_out&#x27;: 200, &#x27;num_env_steps_trained_lifetime&#x27;: 200, &#x27;num_trainable_parameters&#x27;: 655878, &#x27;num_module_steps_trained&#x27;: 404, &#x27;num_env_steps_trained&#x27;: 200, &#x27;num_module_steps_trained_lifetime&#x27;: 404, &#x27;num_env_steps_trained_lifetime_throughput&#x27;: nan}, &#x27;p0&#x27;: {&#x27;num_trainable_parameters&#x27;: 655878, &#x27;weights_seq_no&#x27;: 1.0, &#x27;entropy&#x27;: 1.5917836427688599, &#x27;module_train_batch_size_mean&#x27;: 404, &#x27;policy_loss&#x27;: -0.15537874400615692, &#x27;total_loss&#x27;: -0.15024973452091217, &#x27;vf_loss_unclipped&#x27;: 1.6708769123852107e-07, &#x27;num_module_steps_trained&#x27;: 404, &#x27;mean_kl_loss&#x27;: 0.025644313544034958, &#x27;num_module_steps_trained_lifetime&#x27;: 404, &#x27;curr_kl_coeff&#x27;: 0.30000001192092896, &#x27;num_non_trainable_parameters&#x27;: 0, &#x27;vf_explained_var&#x27;: 0.9406133890151978, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;gradients_default_optimizer_global_norm&#x27;: 0.9358460307121277, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.0, &#x27;vf_loss&#x27;: 1.6708769123852107e-07}}</td><td style=\"text-align: right;\">                             200</td><td style=\"text-align: right;\">                                        nan</td><td style=\"text-align: right;\">                                      1</td><td>{&#x27;cpu_util_percent&#x27;: 67.225, &#x27;ram_util_percent&#x27;: 96.525}</td><td>{&#x27;training_iteration&#x27;: 2.536862332985038, &#x27;restore_env_runners&#x27;: 2.4582986952736974e-05, &#x27;training_step&#x27;: 2.5366032080200966, &#x27;env_runner_sampling_timer&#x27;: 0.5369193330116104, &#x27;learner_update_timer&#x27;: 1.9899427500204183, &#x27;synch_weights&#x27;: 0.009463166003115475}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:51:50,353\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/che/ray_results/PPO_2025-03-21_16-51-28' in 0.0099s.\n",
      "\u001b[36m(PPO(env=flatland_env; env-runners=2; learners=0; multi-agent=True) pid=85022)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/che/ray_results/PPO_2025-03-21_16-51-28/PPO_flatland_env_59ef6_00000_0_2025-03-21_16-51-28/checkpoint_000000)\n",
      "2025-03-21 16:51:50,489\tINFO tune.py:1041 -- Total run time: 22.13 seconds (21.97 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-03-21 16:51:50 (running for 00:00:21.98)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-03-21_16-51-24_808186_84981/artifacts/2025-03-21_16-51-28/PPO_2025-03-21_16-51-28/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+------------------------------+------------+-----------------+--------+------------------+------+\n",
      "| Trial name                   | status     | loc             |   iter |   total time (s) |   ts |\n",
      "|------------------------------+------------+-----------------+--------+------------------+------|\n",
      "| PPO_flatland_env_59ef6_00000 | TERMINATED | 127.0.0.1:85022 |      1 |          2.54778 |  200 |\n",
      "+------------------------------+------------+-----------------+--------+------------------+------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=85027)\u001b[0m /Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:321: UserWarning: Could not set all required cities! Created 1/2\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85027)\u001b[0m   warnings.warn(city_warning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85027)\u001b[0m /Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:217: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=85027)\u001b[0m   warnings.warn(\"[WARNING] Changing to Grid mode to place at least 2 cities.\")\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=85022)\u001b[0m 2025-03-21 16:51:46,512\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=85022)\u001b[0m 2025-03-21 16:51:46,472\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=85022)\u001b[0m agent_to_module_mapping.py: [Box(0.0, 2.0, (1020,), float64), Box(0.0, 2.0, (1020,), float64)]\u001b[32m [repeated 12x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "algo = \"PPO\"\n",
    "obid = \"FlattenedNormalizedTreeObsForRailEnv_max_depth_3_50\"\n",
    "# in order to get the results, we call `train()` directly from python\n",
    "results = train(parser.parse_args(\n",
    "    [\"--num-agents\", \"2\", \"--obs-builder\", obid, \"--algo\", algo, \"--stop-iters\", \"1\", \"--train-batch-size-per-learner\", \"200\", \"--checkpoint-freq\", \"1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91accab8-1e7a-46d3-af0a-30db503cd0c6",
   "metadata": {},
   "source": [
    "## Rollout from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732c342f-5cb7-492d-93e4-352ce5b5e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = add_flatland_inference_with_random_policy_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20c462-0361-4031-8e71-ef90cb0c6ffb",
   "metadata": {},
   "source": [
    "#### Inspect Rollout cli Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489a7b54-d5d6-4c52-92e6-a709068b02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: flatland_inference_with_random_policy.py [-h] [--num-agents NUM_AGENTS]\n",
      "                                                [--obs-builder OBS_BUILDER]\n",
      "                                                [--num-episodes-during-inference NUM_EPISODES_DURING_INFERENCE]\n",
      "                                                [--policy-id POLICY_ID]\n",
      "                                                [--cp CP]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --num-agents NUM_AGENTS\n",
      "  --obs-builder OBS_BUILDER\n",
      "  --num-episodes-during-inference NUM_EPISODES_DURING_INFERENCE\n",
      "                        Number of episodes to do inference over (after\n",
      "                        restoring from a checkpoint).\n",
      "  --policy-id POLICY_ID\n",
      "  --cp CP\n"
     ]
    }
   ],
   "source": [
    "!python -m flatland.ml.ray.examples.flatland_inference_with_random_policy --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649271c6-a636-479f-a381-e27afa7579c9",
   "metadata": {},
   "source": [
    "#### Inspect Rollout cli Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b20e15-2709-4a02-8492-425e673da9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Create an env to do inference in.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_env_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_builder_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregistry_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_builder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepisode_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"learner_group\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"learner\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"rl_module\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrl_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRLModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrl_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomRLModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes_during_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mobss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrl_module_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mColumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACTIONS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrl_module_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0maction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_module_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_module_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACTION_DIST_INPUTS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0maction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRailEnvActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0maction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0maction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminateds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncateds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Is the episode `done`? -> Reset.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mterminateds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"__all__\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncateds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"__all__\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mEpisode done: Total reward = \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mepisode_return\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnum_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mepisode_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mDone performing action inference through \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Episodes\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat inspect.getsource(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2047a7-d7ca-4df8-8298-9754f41d92f6",
   "metadata": {},
   "source": [
    "#### Rollout on best checkpoint from previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62fbd3ab-a645-4877-abcb-f353e75f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_flatland_ray_cli_observation_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fb59fd-a221-4db2-842a-7a7e3c7ccec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(\n",
    "    metric=f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0418274-fc2f-48f4-b2ea-502c4cff2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:321: UserWarning: Could not set all required cities! Created 1/2\n",
      "  warnings.warn(city_warning)\n",
      "/Users/che/workspaces/flatland-rl-2/flatland/envs/rail_generators.py:217: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.\n",
      "  warnings.warn(\"[WARNING] Changing to Grid mode to place at least 2 cities.\")\n",
      "2025-03-21 16:52:03,227\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "Episode done: Total reward = -230.0\n",
      "Done performing action inference through 1 Episodes\n"
     ]
    }
   ],
   "source": [
    "!python -m flatland.ml.ray.examples.flatland_inference_with_random_policy --num-agents 2 --obs-builder {obid} --cp {best_result.checkpoint.path} --policy-id p0  --num-episodes-during-inference 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
